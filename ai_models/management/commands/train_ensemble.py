# ai_models/management/commands/train_ensemble.py
import os
import random
import joblib
import numpy as np
from pathlib import Path
from django.core.management.base import BaseCommand

# sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.calibration import CalibratedClassifierCV
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

# tensorflow / keras
import tensorflow as tf
from tensorflow.keras import layers, models, callbacks

# Where to save artifacts
BASE_DIR = Path("ai_models/trained")
BASE_DIR.mkdir(parents=True, exist_ok=True)

# ---------------- Synthetic data generator (better balance + noise) ----------------
def generate_tabular_and_sequences(n_samples=5000, seq_len=30, pos_frac=0.25):
    """
    Generate synthetic dataset:
      - X_tabular: aggregates
      - X_seq: sequences (seq_len x obs_dim)
      - X_img: synthetic radar-like images
      - y_thunder, y_gale: labels (balanced-ish, with noise)
    Improvements:
      - attempt to keep thunder/gale fractions reasonable (pos_frac)
      - inject label noise to avoid perfect deterministic rules
    """
    obs_dim = 5
    img_h, img_w = 32, 32

    X_tab = []
    X_seq = []
    X_img = []
    y_thunder = []
    y_gale = []

    # generate sequences and compute "rule score", then sample to get desired pos_frac
    pool = []
    for i in range(n_samples * 3):  # generate a pool larger than n_samples
        temps = np.random.normal(loc=30, scale=4, size=seq_len)
        hums = np.clip(np.random.normal(loc=60, scale=20, size=seq_len), 5, 100)
        press = np.random.normal(loc=1005, scale=6, size=seq_len)
        wind = np.abs(np.random.normal(loc=6, scale=4, size=seq_len))
        gust = wind + np.abs(np.random.normal(loc=2, scale=4, size=seq_len))

        seq = np.stack([temps, hums, press, wind, gust], axis=1)
        # crude rule scores
        thunder_score = (seq[-1,1] - 60) / 40.0 + (seq[-1,0] - 28) / 8.0 + (1005 - seq[-1,2]) / 8.0
        gale_score = (seq[:,4].max() / 25.0) + (seq[:,3].mean() / 20.0)
        pool.append((seq.astype(np.float32), thunder_score, gale_score))

    # sort pool by thunder_score for controlled sampling
    pool_sorted = sorted(pool, key=lambda x: x[1], reverse=True)

    chosen = pool_sorted[:n_samples]
    for seq, thunder_score, gale_score in chosen:
        # create tabular aggregates
        feats = []
        feats += [seq[:, 0].mean(), seq[:, 0].std(), seq[:, 0].max()]  # temp
        feats += [seq[:, 1].mean(), seq[:, 1].std()]  # humidity
        feats += [seq[:, 2].mean(), seq[:, 2].std()]  # pressure
        feats += [seq[:, 3].mean(), seq[:, 3].max()]  # wind
        feats += [seq[:, 4].max()]  # peak gust

        # synthetic radar image (gaussian blobs)
        img = np.zeros((img_h, img_w), dtype=np.float32)
        n_blobs = random.randint(1, 4)
        for b in range(n_blobs):
            cx = random.uniform(6, img_w - 6)
            cy = random.uniform(6, img_h - 6)
            sigma = random.uniform(2.0, 6.0)
            intensity = np.clip((seq[-1,1] / 100.0) * (1 + seq[-1,4] / 30.0) + random.uniform(-0.15, 0.15), 0, 1)
            xs = np.arange(img_w)
            ys = np.arange(img_h)[:, None]
            g = np.exp(-((xs - cx)**2 + (ys - cy)**2) / (2*sigma**2))
            img += intensity * g
        img = img / (img.max() + 1e-8)
        img = np.clip(img, 0, 1)

        # label rules but with noise
        thunder_label = 1 if (thunder_score > 0.3) else 0
        gale_label = 1 if (gale_score > 0.9) else 0

        # inject some noise (flip labels at low probability)
        if random.random() < 0.08:
            thunder_label = 1 - thunder_label
        if random.random() < 0.05:
            gale_label = 1 - gale_label

        X_tab.append(np.array(feats, dtype=np.float32))
        X_seq.append(seq)
        X_img.append(img[..., None].astype(np.float32))
        y_thunder.append(thunder_label)
        y_gale.append(gale_label)

    return (np.stack(X_tab), np.stack(X_seq), np.stack(X_img),
            np.array(y_thunder, dtype=np.int32), np.array(y_gale, dtype=np.int32))


# ---------------- Models builders ----------------
def build_lstm_model(seq_len=30, obs_dim=5):
    inp = layers.Input(shape=(seq_len, obs_dim), name="lstm_input")
    x = layers.Masking()(inp)
    x = layers.LSTM(64, return_sequences=True)(x)
    x = layers.LSTM(32)(x)
    x = layers.Dense(32, activation="relu")(x)
    out = layers.Dense(1, activation="sigmoid", name="thunder_out")(x)
    model = models.Model(inputs=inp, outputs=out)
    model.compile(optimizer="adam", loss="binary_crossentropy", metrics=[tf.keras.metrics.AUC(name="AUC")])
    return model


def build_cnn_model(h=32, w=32):
    inp = layers.Input(shape=(h, w, 1), name="cnn_input")
    x = layers.Conv2D(16, 3, activation="relu", padding="same")(inp)
    x = layers.MaxPool2D()(x)
    x = layers.Conv2D(32, 3, activation="relu", padding="same")(x)
    x = layers.MaxPool2D()(x)
    x = layers.Conv2D(64, 3, activation="relu", padding="same")(x)
    x = layers.GlobalAveragePooling2D()(x)
    x = layers.Dense(64, activation="relu")(x)
    out = layers.Dense(1, activation="sigmoid", name="cnn_out")(x)
    model = models.Model(inputs=inp, outputs=out)
    model.compile(optimizer="adam", loss="binary_crossentropy", metrics=[tf.keras.metrics.AUC(name="AUC")])
    return model


class Command(BaseCommand):
    help = "Train RF + LSTM + CNN ensemble on improved synthetic weather data and save artifacts"

    def add_arguments(self, parser):
        parser.add_argument("--samples", type=int, default=5000, help="Number of synthetic samples")
        parser.add_argument("--seq-len", type=int, default=30, help="Sequence length for LSTM")
        parser.add_argument("--epochs", type=int, default=8, help="Epochs for LSTM/CNN training")
        parser.add_argument("--batch", type=int, default=64, help="Batch size")

    def handle(self, *args, **options):
        n = int(options["samples"])
        seq_len = int(options["seq_len"])
        epochs = int(options["epochs"])
        batch = int(options["batch"])

        self.stdout.write(self.style.NOTICE(f"Generating {n} synthetic samples (seq_len={seq_len})..."))
        X_tab, X_seq, X_img, y_th, y_gale = generate_tabular_and_sequences(n, seq_len)

        # split once, keep alignment
        (X_tab_tr, X_tab_te,
         X_seq_tr, X_seq_te,
         X_img_tr, X_img_te,
         y_tr_th, y_te_th) = train_test_split(
            X_tab, X_seq, X_img, y_th, test_size=0.2, random_state=42)

        # To get matching gale splits for meta assembly, split y_gale with same indices:
        _, _, _, _, _, _, y_tr_gale, y_te_gale = train_test_split(
            X_tab, X_seq, X_img, y_gale, test_size=0.2, random_state=42)

        # ---------------- RandomForest (tabular) ----------------
        self.stdout.write(self.style.NOTICE("Training RandomForest (tabular) for Thunder..."))
        rf_th = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1, class_weight="balanced")
        rf_th.fit(X_tab_tr, y_tr_th)
        # calibrate RF probabilities (better proba)
        calib_rf_th = CalibratedClassifierCV(rf_th, cv=5)
        calib_rf_th.fit(X_tab_tr, y_tr_th)
        p_rf_th = calib_rf_th.predict_proba(X_tab_te)[:, 1]
        auc_rf_th = roc_auc_score(y_te_th, p_rf_th)
        self.stdout.write(self.style.SUCCESS(f"RF(thunder) AUC: {auc_rf_th:.3f}"))

        self.stdout.write(self.style.NOTICE("Training RandomForest (tabular) for Gale..."))
        rf_gale = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1, class_weight="balanced")
        rf_gale.fit(X_tab_tr, y_tr_gale)
        calib_rf_g = CalibratedClassifierCV(rf_gale, cv=5)
        calib_rf_g.fit(X_tab_tr, y_tr_gale)
        p_rf_g = calib_rf_g.predict_proba(X_tab_te)[:, 1]
        auc_rf_g = roc_auc_score(y_te_gale, p_rf_g)
        self.stdout.write(self.style.SUCCESS(f"RF(gale) AUC: {auc_rf_g:.3f}"))

        # save calibrated RF wrappers
        joblib.dump(calib_rf_th, str(BASE_DIR / "rf_thunder_calib.joblib"))
        joblib.dump(calib_rf_g, str(BASE_DIR / "rf_gale_calib.joblib"))

        # ---------------- LSTM (sequence -> thunder) ----------------
        self.stdout.write(self.style.NOTICE("Training LSTM (sequence) for thunder..."))
        lstm_th = build_lstm_model(seq_len=seq_len, obs_dim=X_seq.shape[2])
        es = callbacks.EarlyStopping(monitor="val_loss", patience=3, restore_best_weights=True)
        lstm_th.fit(X_seq_tr, y_tr_th, validation_data=(X_seq_te, y_te_th),
                    epochs=epochs, batch_size=batch, callbacks=[es], verbose=2)
        p_lstm_th = lstm_th.predict(X_seq_te).ravel()
        auc_lstm_th = roc_auc_score(y_te_th, p_lstm_th)
        self.stdout.write(self.style.SUCCESS(f"LSTM thunder AUC: {auc_lstm_th:.3f}"))
        # save as modern Keras format to avoid legacy HDF5 issues
        lstm_th.save(str(BASE_DIR / "lstm_thunder.keras"))

        # ---------------- CNN (radar image -> thunder) ----------------
        self.stdout.write(self.style.NOTICE("Training CNN (radar image) for thunder..."))
        cnn_th = build_cnn_model(h=X_img.shape[1], w=X_img.shape[2])
        es2 = callbacks.EarlyStopping(monitor="val_loss", patience=3, restore_best_weights=True)
        cnn_th.fit(X_img_tr, y_tr_th, validation_data=(X_img_te, y_te_th),
                   epochs=epochs, batch_size=batch, callbacks=[es2], verbose=2)
        p_cnn_th = cnn_th.predict(X_img_te).ravel()
        auc_cnn_th = roc_auc_score(y_te_th, p_cnn_th)
        self.stdout.write(self.style.SUCCESS(f"CNN thunder AUC: {auc_cnn_th:.3f}"))
        cnn_th.save(str(BASE_DIR / "cnn_thunder.keras"))

        # ---------------- Meta-learner for thunder ----------------
        self.stdout.write(self.style.NOTICE("Building meta-learner (logistic regression) for thunder..."))
        rf_te_th = calib_rf_th.predict_proba(X_tab_te)[:, 1]
        lstm_te_th = lstm_th.predict(X_seq_te).ravel()
        cnn_te_th = cnn_th.predict(X_img_te).ravel()
        meta_X = np.vstack([rf_te_th, lstm_te_th, cnn_te_th]).T
        meta_y = y_te_th
        meta = LogisticRegression(max_iter=2000)
        meta.fit(meta_X, meta_y)
        meta_p = meta.predict_proba(meta_X)[:, 1]
        auc_meta = roc_auc_score(meta_y, meta_p)
        self.stdout.write(self.style.SUCCESS(f"Meta-learner AUC (thunder holdout): {auc_meta:.3f}"))
        joblib.dump(meta, str(BASE_DIR / "meta_thunder.joblib"))

        # ---------------- Simple meta for gale (tabular RF already calibrated) ----------------
        # For gale we'll use RF calibrated directly but also train small logistic on RF probs to calibrate further
        self.stdout.write(self.style.NOTICE("Building meta for gale (logistic on RF probabilities)..."))
        meta_gale = LogisticRegression(max_iter=1000)
        meta_gale.fit(p_rf_g.reshape(-1, 1), y_te_gale)  # small meta fitted on holdout distribution
        joblib.dump(meta_gale, str(BASE_DIR / "meta_gale.joblib"))

        # Save RF base models (optional too)
        joblib.dump(rf_th, str(BASE_DIR / "rf_thunder_base.joblib"))
        joblib.dump(rf_gale, str(BASE_DIR / "rf_gale_base.joblib"))

        self.stdout.write(self.style.SUCCESS("âœ… Ensemble training complete. Models saved:"))
        self.stdout.write(str(BASE_DIR))
        self.stdout.write(self.style.SUCCESS(
            "Files: rf_thunder_calib.joblib, rf_gale_calib.joblib, lstm_thunder.keras, cnn_thunder.keras, meta_thunder.joblib, meta_gale.joblib"
        ))
